{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4WUTHslU81KH"},"outputs":[],"source":["# 데이터 zip파일 압축 풀기\n","!unzip './drive/MyDrive/week11/data/cats_and_dogs.zip' -d '/content'"]},{"cell_type":"code","source":["from torchvision.io import read_image\n","from torch.utils.data import Dataset\n","import pandas as pd\n","from torch.utils.data import DataLoader"],"metadata":{"id":"3RbD3RovBWjF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 데이터 불러오는 함수\n","class CustomImageDataset(Dataset):\n","  def __init__(self,img_dir,transform=None):\n","    self.img_labels = pd.read_csv(f'{img_dir}/label.csv')\n","    self.img_dir = img_dir\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return len(self.img_labels)\n","\n","  def __getitem__(self,idx):\n","    img_path = f'{self.img_dir}/{self.img_labels.iloc[idx,0]}'\n","    img = read_image(img_path)\n","    if self.transform:\n","      img = self.transform(img)\n","    label = self.img_labels.iloc[idx,1]\n","    return img,label"],"metadata":{"id":"8ElPCXse59BG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지 사이즈 단일화 및 augmentation\n","from torchvision.transforms import transforms, Resize\n","from torchvision.transforms import RandomCrop,RandomRotation,RandomHorizontalFlip\n","\n","\n","desired_size = 256\n","transform = transforms.Compose([\n","    Resize(size=desired_size),\n","    RandomCrop(desired_size),\n","    RandomHorizontalFlip(),\n","    RandomRotation(degrees=(-45,45))\n","])"],"metadata":{"id":"qVyu5c-NSGu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 정의\n","train_dataset = CustomImageDataset(\n","    img_dir = '/content/cats_and_dogs/train',\n","    transform = transform,\n","    )\n","\n","test_dataset = CustomImageDataset(\n","    img_dir = '/content/cats_and_dogs/test',\n","    transform = transform\n","    )"],"metadata":{"id":"HxcazRzMSFb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 256\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"metadata":{"id":"sb04swTr-o5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","class SimpleCNN(torch.nn.Module):\n","  def __init__(self):\n","    super(SimpleCNN,self).__init__()\n","    self.conv1 = torch.nn.Conv2d(in_channels=3,out_channels=4,kernel_size=3,padding='same')\n","    self.conv2 = torch.nn.Conv2d(in_channels=4,out_channels=8,kernel_size=3,padding='same')\n","    self.conv3 = torch.nn.Conv2d(in_channels=8,out_channels=16,kernel_size=3,padding='same')\n","    self.fc = torch.nn.Linear(in_features=16384, out_features=10)\n","    self.pool = torch.nn.MaxPool2d(2,2)\n","    self.activation = torch.nn.ReLU()\n","\n","  def forward(self,x):\n","    x = self.pool(self.activation(self.conv1(x)))\n","    x = self.pool(self.activation(self.conv2(x)))\n","    x = self.pool(self.activation(self.conv3(x)))\n","    x = torch.flatten(x,1)\n","    x = self.fc(x)\n","    return x"],"metadata":{"id":"SldhK1k259Nw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = SimpleCNN().to(device)"],"metadata":{"id":"V13gRQ8W59QD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.001\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","criterion = torch.nn.CrossEntropyLoss()\n","epochs = 10"],"metadata":{"id":"5K2A7Ofb9VD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_mean(metrics):\n","    return round(sum(metrics) / len(metrics), 4)"],"metadata":{"id":"60fkXhMM98Id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","\n","\n","def train_model(model):\n","    model.train()\n","    loss_list = []\n","    acc_list = []\n","    for x_train, y_train in tqdm(train_dataloader):\n","        x_train = x_train.to(device).float()\n","        y_train = y_train.to(device)\n","\n","        outputs = model(x_train)\n","        loss = criterion(outputs, y_train)\n","        loss_list.append(loss.item())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = torch.argmax(outputs, dim=1)`\n","        acc = ((y_train == pred).sum() / len(y_train)).item()\n","        acc_list.append(acc)\n","    return get_mean(loss_list), get_mean(acc_list)"],"metadata":{"id":"MiQ8RWMJ-ITd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_model(model):\n","    model.eval()\n","    loss_list = []\n","    acc_list = []\n","    for x_val, y_val in tqdm(test_dataloader):\n","        x_val = x_val.to(device).float()\n","        y_val = y_val.to(device)\n","        with torch.no_grad():\n","            outputs = model(x_val)\n","            loss = criterion(outputs, y_val)\n","            loss_list.append(loss.item())\n","\n","            pred = torch.argmax(outputs, dim=1)\n","            acc = ((y_val == pred).sum() / len(y_val)).item()\n","            acc_list.append(acc)\n","    return get_mean(loss_list), get_mean(acc_list)"],"metadata":{"id":"Y3Rh_3XX-Kc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","\n","def train_validate_model(model):\n","    logs = defaultdict(list)\n","    for epoch in range(epochs):\n","        train_loss, train_acc = train_model(model)\n","        val_loss, val_acc = validate_model(model)\n","        logs[\"train_loss\"].append(train_loss)\n","        logs[\"train_acc\"].append(train_acc)\n","        logs[\"val_loss\"].append(val_loss)\n","        logs[\"val_acc\"].append(val_acc)\n","        print(f\"epoch {epoch + 1} train - loss: {train_loss} acc: {train_acc} val - loss: {val_loss} acc: {val_acc}\")\n","    return logs"],"metadata":{"id":"BENzFK1W-M6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logs = train_validate_model(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SuAYnyG2-Ocl","executionInfo":{"status":"ok","timestamp":1706446693598,"user_tz":-540,"elapsed":1041679,"user":{"displayName":"‍권기훈[학생](체육대학 스포츠의학과)","userId":"00869514058462903422"}},"outputId":"77b8e33f-a2d9-4999-ea12-99a00ad4ea5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","100%|██████████| 32/32 [01:24<00:00,  2.66s/it]\n","100%|██████████| 8/8 [00:18<00:00,  2.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 train - loss: 2.5294 acc: 0.521 val - loss: 0.7152 acc: 0.5668\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:24<00:00,  2.65s/it]\n","100%|██████████| 8/8 [00:19<00:00,  2.43s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 train - loss: 0.6679 acc: 0.6014 val - loss: 0.6679 acc: 0.5963\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:23<00:00,  2.60s/it]\n","100%|██████████| 8/8 [00:21<00:00,  2.70s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 train - loss: 0.6522 acc: 0.6133 val - loss: 0.6456 acc: 0.6261\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:24<00:00,  2.64s/it]\n","100%|██████████| 8/8 [00:21<00:00,  2.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 4 train - loss: 0.6282 acc: 0.653 val - loss: 0.6221 acc: 0.6567\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:24<00:00,  2.65s/it]\n","100%|██████████| 8/8 [00:20<00:00,  2.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 5 train - loss: 0.6047 acc: 0.676 val - loss: 0.6234 acc: 0.6599\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:24<00:00,  2.65s/it]\n","100%|██████████| 8/8 [00:20<00:00,  2.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 6 train - loss: 0.595 acc: 0.6855 val - loss: 0.6038 acc: 0.6851\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:24<00:00,  2.63s/it]\n","100%|██████████| 8/8 [00:19<00:00,  2.47s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 7 train - loss: 0.5965 acc: 0.686 val - loss: 0.607 acc: 0.6792\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:23<00:00,  2.60s/it]\n","100%|██████████| 8/8 [00:21<00:00,  2.67s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 8 train - loss: 0.5854 acc: 0.7017 val - loss: 0.5863 acc: 0.6931\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:22<00:00,  2.58s/it]\n","100%|██████████| 8/8 [00:19<00:00,  2.46s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch 9 train - loss: 0.5725 acc: 0.7057 val - loss: 0.5803 acc: 0.6997\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 32/32 [01:22<00:00,  2.59s/it]\n","100%|██████████| 8/8 [00:19<00:00,  2.46s/it]"]},{"output_type":"stream","name":"stdout","text":["epoch 10 train - loss: 0.5704 acc: 0.7092 val - loss: 0.5868 acc: 0.6823\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5rqxfxr9-fH2"},"execution_count":null,"outputs":[]}]}